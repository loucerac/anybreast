{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAZ7M__ncb8Y"
   },
   "source": [
    "# Notebook: fine-tune SAM (segment anything) on a custom dataset\n",
    "\n",
    "In this notebook, we'll reproduce the [MedSAM](https://github.com/bowang-lab/MedSAM) project, which fine-tunes [SAM](https://huggingface.co/docs/transformers/main/en/model_doc/sam) on a dataset of medical images. For demo purposes, we'll use a toy dataset, but this can easily be scaled up.\n",
    "\n",
    "Resources used to create this notebook (thanks 🙏):\n",
    "* [Encode blog post](https://encord.com/blog/learn-how-to-fine-tune-the-segment-anything-model-sam/)\n",
    "* [MedSAM repository](https://github.com/bowang-lab/MedSAM).\n",
    "\n",
    "## Set-up environment\n",
    "\n",
    "We first install 🤗 Transformers and 🤗 Datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RhFPJgpIbVat"
   },
   "source": [
    "We also install the [Monai](https://github.com/Project-MONAI/MONAI) repository as we'll use a custom loss function from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ROd15m4Ucdut"
   },
   "source": [
    "## Load dataset\n",
    "\n",
    "Here we load a small dataset of 130 (image, ground truth mask) pairs.\n",
    "\n",
    "To load your own images and masks, refer to the bottom of my [SAM inference notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/SAM/Run_inference_with_MedSAM_using_HuggingFace_Transformers.ipynb).\n",
    "\n",
    "See also [this guide](https://huggingface.co/docs/datasets/image_dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kRf-WnHqcbcq"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carlos/github/smaibreast/.pixi/envs/default/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_orig = load_dataset(\"nielsr/breast-cancer\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetInfo(description='', citation='', homepage='', license='', features={'image': Image(mode=None, decode=True, id=None), 'label': Image(mode=None, decode=True, id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name='parquet', dataset_name='breast-cancer', config_name='default', version=0.0.0, splits={'train': SplitInfo(name='train', num_bytes=42432726, num_examples=130, shard_lengths=None, dataset_name='breast-cancer')}, download_checksums={'hf://datasets/nielsr/breast-cancer@4cad8ecefb61ef407b4e932f23533196e44c86ba/data/train-00000-of-00001-9cd7b7429038b476.parquet': {'num_bytes': 10004141, 'checksum': None}}, download_size=10004141, post_processing_size=None, dataset_size=42432726, size_in_bytes=52436867)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_orig.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241m.\u001b[39mtrain_test_split(test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "dataset = dataset.train_test_split(test_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kJ58CT8wdsPd"
   },
   "outputs": [],
   "source": [
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gtP_Xwol1MlA"
   },
   "source": [
    "We can visualize an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uTchd20g1Nri"
   },
   "outputs": [],
   "source": [
    "example = dataset[\"train\"][0]\n",
    "image = example[\"image\"]\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DwWcPvK11nAP"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "fig, axes = plt.subplots()\n",
    "\n",
    "axes.imshow(np.array(image))\n",
    "ground_truth_seg = np.array(example[\"label\"])\n",
    "show_mask(ground_truth_seg, axes)\n",
    "axes.title.set_text(f\"Ground truth mask\")\n",
    "axes.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2cOa80Qc3FQ"
   },
   "source": [
    "## Create PyTorch dataset\n",
    "\n",
    "Below we define a regular PyTorch dataset, which gives us examples of the data prepared in the format for the model. Each example consists of:\n",
    "\n",
    "* pixel values (which is the image prepared for the model)\n",
    "* a prompt in the form of a bounding box\n",
    "* a ground truth segmentation mask.\n",
    "\n",
    "The function below defines how to get a bounding box prompt based on the ground truth segmentation. This was taken from [here](https://github.com/bowang-lab/MedSAM/blob/66cf4799a9ab9a8e08428a5087e73fc21b2b61cd/train.py#L29).\n",
    "\n",
    "Note that SAM is always trained using certain \"prompts\", which you could be bounding boxes, points, text, or rudimentary masks. The model is then trained to output the appropriate mask given the image + prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5LWBu8d1egig"
   },
   "outputs": [],
   "source": [
    "def get_bounding_box(ground_truth_map):\n",
    "  # get bounding box from mask\n",
    "  y_indices, x_indices = np.where(ground_truth_map > 0)\n",
    "  x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
    "  y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
    "  # add perturbation to bounding box coordinates\n",
    "  H, W = ground_truth_map.shape\n",
    "  x_min = max(0, x_min - np.random.randint(0, 20))\n",
    "  x_max = min(W, x_max + np.random.randint(0, 20))\n",
    "  y_min = max(0, y_min - np.random.randint(0, 20))\n",
    "  y_max = min(H, y_max + np.random.randint(0, 20))\n",
    "  bbox = [x_min, y_min, x_max, y_max]\n",
    "\n",
    "  return bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zyC58ImHc2vO"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SAMDataset(Dataset):\n",
    "  def __init__(self, dataset, processor):\n",
    "    self.dataset = dataset\n",
    "    self.processor = processor\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.dataset)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    item = self.dataset[idx]\n",
    "    image = item[\"image\"]\n",
    "    ground_truth_mask = np.array(item[\"label\"])\n",
    "\n",
    "    # get bounding box prompt\n",
    "    prompt = get_bounding_box(ground_truth_mask)\n",
    "\n",
    "    # prepare image and prompt for the model\n",
    "    inputs = self.processor(image, input_boxes=[[prompt]], return_tensors=\"pt\")\n",
    "\n",
    "    # remove batch dimension which the processor adds by default\n",
    "    inputs = {k:v.squeeze(0) for k,v in inputs.items()}\n",
    "\n",
    "    # add ground truth segmentation\n",
    "    inputs[\"ground_truth_mask\"] = ground_truth_mask\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wE4iTOZdeLjq"
   },
   "outputs": [],
   "source": [
    "from transformers import SamProcessor\n",
    "\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XTynfgToe8jj"
   },
   "outputs": [],
   "source": [
    "train_dataset = SAMDataset(dataset=dataset[\"train\"], processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dYZKU4iBfB-E"
   },
   "outputs": [],
   "source": [
    "example = train_dataset[0]\n",
    "for k,v in example.items():\n",
    "  print(k,v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xyD4kOCFf76Q"
   },
   "source": [
    "## Create PyTorch DataLoader\n",
    "\n",
    "Next we define a PyTorch Dataloader, which allows us to get batches from the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9CUnLOjSf9Kn"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5D2bmjAhgIus"
   },
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "for k,v in batch.items():\n",
    "  print(k,v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eJyuJc7fOldT"
   },
   "outputs": [],
   "source": [
    "batch[\"ground_truth_mask\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQx2Aq7LeAMU"
   },
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZI2ioeS5eAxm"
   },
   "outputs": [],
   "source": [
    "from transformers import SamModel\n",
    "\n",
    "model = SamModel.from_pretrained(\"facebook/sam-vit-base\")\n",
    "\n",
    "# make sure we only compute gradients for mask decoder\n",
    "for name, param in model.named_parameters():\n",
    "  if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CShhxC-heDpw"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vOTrMv1LeEK9"
   },
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "import monai\n",
    "\n",
    "# Note: Hyperparameter tuning could improve performance here\n",
    "optimizer = Adam(model.mask_decoder.parameters(), lr=1e-5, weight_decay=0)\n",
    "\n",
    "seg_loss = monai.losses.DiceCELoss(sigmoid=True, squared_pred=True, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XC35CzLxfdQU"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from statistics import mean\n",
    "\n",
    "from torch.nn.functional import threshold, normalize\n",
    "\n",
    "num_epochs = 20\n",
    "model.to(device)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_losses = []\n",
    "    for batch in tqdm(train_dataloader):\n",
    "      # forward pass\n",
    "      outputs = model(pixel_values=batch[\"pixel_values\"].to(device),\n",
    "                      input_boxes=batch[\"input_boxes\"].to(device),\n",
    "                      multimask_output=False)\n",
    "\n",
    "      # compute loss\n",
    "      predicted_masks = outputs.pred_masks.squeeze(1)\n",
    "      ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device)\n",
    "      loss = seg_loss(predicted_masks, ground_truth_masks.unsqueeze(1))\n",
    "\n",
    "      # backward pass (compute gradients of parameters w.r.t. loss)\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "\n",
    "      # optimize\n",
    "      optimizer.step()\n",
    "      epoch_losses.append(loss.item())\n",
    "\n",
    "    print(f'EPOCH: {epoch}')\n",
    "    print(f'Mean loss: {mean(epoch_losses)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IYKvlFthRsyG"
   },
   "source": [
    "## Inference\n",
    "\n",
    "Important note here: as we used the Dice loss with `sigmoid=True`, we need to make sure to appropriately apply a sigmoid activation function to the predicted masks. Hence we won't use the processor's `post_process_masks` method here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nyES0ru3aiVf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# let's take a random training example\n",
    "idx = 0\n",
    "\n",
    "# load image\n",
    "image = dataset[\"test\"][idx][\"image\"]\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OzxIzE6janRx"
   },
   "outputs": [],
   "source": [
    "# get box prompt based on ground truth segmentation map\n",
    "ground_truth_mask = np.array(dataset[\"test\"][idx][\"label\"])\n",
    "prompt = get_bounding_box(ground_truth_mask)\n",
    "\n",
    "# prepare image + box prompt for the model\n",
    "inputs = processor(image, input_boxes=[[prompt]], return_tensors=\"pt\").to(device)\n",
    "for k,v in inputs.items():\n",
    "  print(k,v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OvOVzNvea5oU"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# forward pass\n",
    "with torch.no_grad():\n",
    "  outputs = model(**inputs, multimask_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uvgwHg42bACQ"
   },
   "outputs": [],
   "source": [
    "# apply sigmoid\n",
    "medsam_seg_prob = torch.sigmoid(outputs.pred_masks.squeeze(1))\n",
    "# convert soft mask to hard mask\n",
    "medsam_seg_prob = medsam_seg_prob.cpu().numpy().squeeze()\n",
    "medsam_seg = (medsam_seg_prob > 0.5).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DCehzlJJbDGz"
   },
   "outputs": [],
   "source": [
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10, 10))\n",
    "\n",
    "i = 0\n",
    "axes[i].imshow(np.array(image))\n",
    "axes[i].title.set_text(f\"Original Image\")\n",
    "axes[i].axis(\"off\")\n",
    "\n",
    "i += 1\n",
    "axes[i].imshow(np.array(image))\n",
    "show_mask(ground_truth_mask, axes[i])\n",
    "axes[i].title.set_text(f\"Ground truth mask\")\n",
    "axes[i].axis(\"off\")\n",
    "\n",
    "i += 1\n",
    "axes[i].imshow(np.array(image))\n",
    "show_mask(medsam_seg, axes[i])\n",
    "axes[i].title.set_text(f\"Predicted mask\")\n",
    "axes[i].axis(\"off\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
